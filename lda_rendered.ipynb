{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text\n",
    "\n",
    "There are lots of things you might want to do with documents\n",
    " - group them together\n",
    " - summarize one or several of them\n",
    " - classify them (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what they discuss\n",
    "\n",
    "To do any of these things using a computer, you first need to convert words into numbers.\n",
    "\n",
    "[https://github.com/fastforwardlabs/ldaworkshop](https://github.com/fastforwardlabs/ldaworkshop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bags of words\n",
    "\n",
    "The \"bag of words\" approach is the simplest method for this.\n",
    "\n",
    "![Bag of words](img/bagwords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are at least three problems with this approach:\n",
    "\n",
    " - **long, sparse data**: a short text is turned into a very long series of numbers, most of which are zero (\"sparse\").\n",
    " \n",
    " - **synonyms and multiple meanings**: \"movies\" and \"films\" are different (they should not be), and \"bow\" (that shoots an arrow) and \"bow\" (you make to a King) are not (they should be)\n",
    " \n",
    " - **word order**: totally lost (\"man bites dog\" and \"dog bites man\" are the same).\n",
    "\n",
    "We're going to look at a technique that addresses the first of these two problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling\n",
    "\n",
    "Topic modeling is a statistical method to find groups of words that tend to co-occur in a corpus of documents.\n",
    "\n",
    "For example, maybe the words \"movie\", \"film\" and \"director\" often occur in the same documents. That would make them a \"topic\".\n",
    "\n",
    "Topic modeling algorithms find these groups automatically. They are an instance of the class of algorithms known as \"unsupervised machine learning\".\n",
    "\n",
    "In doing this, we become able to express documents as a combination of a relatively small number (~100) of topics, rather than thousands of words (most of which don't occur), and we can treat documents about \"films\" and \"movies\" similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling workflow\n",
    "\n",
    "Topic modeling has two steps:\n",
    "\n",
    " - learn topics from a corpus of representative documents\n",
    " - figure out which of these topics occur in the particular document(s) you're interested in\n",
    " \n",
    "If you're a machine learning person, you'll recognize these as training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_topics.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_evaluate.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once you've done the second step, you've expressed your new document as a short vector of numbers that you can no do all sorts of things with:\n",
    "\n",
    " - group documents together\n",
    " - summarize one or several documents\n",
    " - classify it (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what people are discussing, e.g. [Time-series plots of 1000 topics extracted from 20 years of the New York Times.](http://christo.cs.umass.edu/NYT/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "The best known and best algorithm for finding topics in a corpus is Latent Dirichlet Allocation. It's got a complicated name and, to be frank, it's a complicated algorithm. If you'd like to begin to dig into the details, there are two resources I recommend very highly!\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
    " \n",
    "For the purposes of this talk, I'm just going to say it finds groups of words that co-occur by magic!\n",
    "\n",
    "The good news is, there are several excellent open source implementations of the algorithm, and we're going to use one of those today.\n",
    "\n",
    "We're going to apply it to a public dataset of Amazon product reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Load data\n",
    "\n",
    "The cell below opens the file and loads it into a pandas dataframe.\n",
    "\n",
    "There's a lot going on in this line, all of which is useful to understand if you're a Python programmer, but none of which is necessary to understand if you're only interested in LDA.\n",
    "\n",
    "If you would like a more detailed explanation of what's going on, please see [the Data notebook](data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "\n",
    "with open(\"reviews.json\", 'rb') as f:\n",
    "    reviews = pd.DataFrame(json.loads(l.decode()) for l in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas dataframe is a structured table-like object that, among many other things, supports a bunch of SQL-like operations and handles fiddly data types like data and times well. I don't know much about R, but I understand R has objects like this too.\n",
    "\n",
    "`head` allows us to see the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I purchased the Trilogy with hoping my two cat...</td>\n",
       "      <td>01 12, 2011</td>\n",
       "      <td>A14CK12J7C7JRK</td>\n",
       "      <td>Consumer in NorCal</td>\n",
       "      <td>Nice Distraction for my cats for about 15 minutes</td>\n",
       "      <td>1294790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>There are usually one or more of my cats watch...</td>\n",
       "      <td>09 14, 2013</td>\n",
       "      <td>A39QHP5WLON5HV</td>\n",
       "      <td>Melodee Placial</td>\n",
       "      <td>Entertaining for my cats</td>\n",
       "      <td>1379116800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  1223000893  [0, 0]      3.0   \n",
       "1  1223000893  [0, 0]      5.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I purchased the Trilogy with hoping my two cat...  01 12, 2011   \n",
       "1  There are usually one or more of my cats watch...  09 14, 2013   \n",
       "\n",
       "       reviewerID        reviewerName  \\\n",
       "0  A14CK12J7C7JRK  Consumer in NorCal   \n",
       "1  A39QHP5WLON5HV     Melodee Placial   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0  Nice Distraction for my cats for about 15 minutes      1294790400  \n",
       "1                           Entertaining for my cats      1379116800  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual columns can be accessed as keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1223000893\n",
       "1    1223000893\n",
       "2    1223000893\n",
       "3    1223000893\n",
       "4    1223000893\n",
       "Name: asin, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['asin'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 reviews\n",
      "of 8019 products \n",
      "by 19834 unique authors \n"
     ]
    }
   ],
   "source": [
    "print(\"{} reviews\".format(len(reviews)))\n",
    "print(\"of {} products \".format(len(reviews.asin.unique())))\n",
    "print(\"by {} unique authors \".format(len(reviews.reviewerID.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = reviews.loc[:19999, 'reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I purchased the Trilogy with hoping my two cats, age 3 and 5 would be interested.  The 3 yr old cat was fascinated for about 15 minutes but when the same pictures came on, she got bored.  The 5 year old watched for about a few minutes but then walked away. It is possible that because we have a wonderful courtyard full of greenery and trees and one of my neighbors has a bird feeder, that there is enough going on outside that they prefer real life versus a taped version.  I will more than likely pass this on to a friend who has cats that don't have as much wildlife to watch as mine do.\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize reviews\n",
    "\n",
    "You have to preprocess the text a little before you apply LDA: you need to split documents into words, and you need to turn words into vectorized numbers.\n",
    "\n",
    "Ironically, in order to get the benefits of LDA, we first need to run bag of words on our data!\n",
    "\n",
    "The code to do this is built into scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 4.07 s, total: 6.33 s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you should do when you fit a topic model is inspect a few of the words that dominate each topic to check that the topics are coherent.\n",
    "\n",
    "To do this, we need to look at the `components_` attribute now attached to `lda`. This is an array with `n_topics` rows and a number of columns equal to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in this array is the weight of the corresponding word in the corresponding topic.\n",
    "\n",
    "The weights of each topic should add up to one, i.e. each row of `lda.components_` should add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4433.27370724,   6655.65363106,   4417.59636293,  11832.90931634,\n",
       "         3963.1514794 ,  10722.5952035 ,   5332.21591946,   3246.91103429,\n",
       "         4706.29046561,  18583.9966215 ,   3390.96693158,   6741.89190383,\n",
       "         4317.35630515,   8728.07765604,   3733.66742834,  19060.45652928,\n",
       "         3249.61949682,   7057.45953921,   3118.25421223,   3886.37900669,\n",
       "         3014.01380836,   3640.80612269,   3879.98580666,  10330.34614701,\n",
       "         3393.53451897,   3318.43067221,   5612.35654271,   3719.26391037,\n",
       "         3191.7666571 ,  12937.90440418,   4010.92549452,   4817.59366305,\n",
       "        14707.85052824,   3811.6920899 ,   6638.30226746,   8799.7396821 ,\n",
       "         4645.7271835 ,   5094.27986301,  17669.6498383 ,   4931.202064  ,\n",
       "         3790.18781725,   6694.45468661,  30470.04926392,   2625.0278297 ,\n",
       "         4988.13237731,   3597.88434063,   3692.79018628,   2972.06107775,\n",
       "         7199.7830463 ,   5628.91583097,   3349.75583582,  13269.54699479,\n",
       "         9543.436802  ,   8423.8673818 ,   3108.00356775,   8606.81503211,\n",
       "         4896.69561975,   5132.21934946,   5318.50737897,   6125.90636408,\n",
       "        11171.63750322,   8626.29704548,   3802.40245259,   2972.16190776,\n",
       "         9206.11651422,  12991.52198047,  10894.10975857,   4347.59823486,\n",
       "         7446.67259541,   3317.29518395,   9166.9177977 ,   3346.97946393,\n",
       "         4424.31109237,   4258.52493162,   9048.6522726 ,   6984.79131057,\n",
       "         9711.55067339,  14635.07323131,   3036.68557328,   6903.25404189,\n",
       "         5980.22293493,   8353.05887838,   4785.90082579,   5153.97306253,\n",
       "         3391.80469183,   3588.49742083,   6772.61783739,   4080.06830134,\n",
       "         4141.93106351,   2973.22239165,  14701.55739324,   3809.74275426,\n",
       "         5753.11754101,   5638.09629268,   5480.06328631,  26214.13843293,\n",
       "         3199.85682329,   3676.99649574,   4747.28455804,   9606.16065047])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear! It turns out there's a bug in scikit-learn's implementation of LDA. Let's fix it here. This should be fixed in the next version of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array of topics and words (or terms) is usually called the topic-term matrix, so let's save it under that name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_term = lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word corresponding to each column in this array, which we'll call the `vocabulary`, is available as a list from `vectorizer.get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', 'bettas', 'contemplating', 'element', 'grunter', 'lays', 'nutro', 'ranchus', 'skinnier', 'tipped']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(vocabulary[:10000:1000]) # print the 0th, 999th, 1999th, 2999th, etc. item in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the top 10 words that dominate each topic.\n",
    "\n",
    "It does that by going through each row (i.e. each topic) in the `topic_term` array, finding the biggest numbers in that row, then finding the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 large small star just think size dog took old kitten\n",
      " 1 bed warm heat night cold pad winter hot hours use\n",
      " 2 product continue use save using koi year old years better\n",
      " 3 litter box clean cat use just pan like don cats\n",
      " 4 just product junk works great piece snails good make using\n",
      " 5 food feeder time feeding day cat feed timer just set\n",
      " 6 air bubbles tubing ear ears just works like pump airline\n",
      " 7 couch long hip energy lasting adorable berries great love summer\n",
      " 8 buy product package great buying disappointed colors good cost im\n",
      " 9 filter filters tank media aquaclear water use canister sponge filtration\n",
      "10 pups dogs love ones cubes make favorite like nylabones ve\n",
      "11 quick use easy great clippers work clean little grip smooth\n",
      "12 dog just bear like milk teddy say really rex little\n",
      "13 ball dog hard treat handle like plastic roll kibble hole\n",
      "14 product did saved away crickets date rid parrot starts rats\n",
      "15 water fish test tank use ammonia kit ph product aquarium\n",
      "16 snake refill don just work receive size ll new like\n",
      "17 smell dog smells dry like shampoo skin bath use good\n",
      "18 material chips like use don kaytee shepard purple seller old\n",
      "19 shepherd german dog jack russell product shepherds love recommend dogs\n",
      "20 rabbit edges color received nice works affordable little sharp wait\n",
      "21 cage fit great holds complaints glass sizes size different love\n",
      "22 small dogs don medium got size thats mini plastic mouths\n",
      "23 food eat brand foods brands eating tried like chicken dog\n",
      "24 vitamin min dried meal max crude calcium sulfate hikari worms\n",
      "25 use really dog disc time dane trying balance just like\n",
      "26 size cans old fit petco petsmart amazon oz bought 13\n",
      "27 34 dog just right received use does way stuff used\n",
      "28 good just recommend nip highly don product knot feel tunnel\n",
      "29 water filter tank clear carbon bag just purigen use gallon\n",
      "30 product ok excellent use toxic know recommend good products used\n",
      "31 bird birds cage perch seed love swing seeds perches parrots\n",
      "32 dogs toy ball dog rope fun lab play love large\n",
      "33 like lol just doesn work dog pills day didn new\n",
      "34 teeth dog dogs toothpaste brush taste flavor breath like loves\n",
      "35 product smell odor spray carpet spot used stain solution miracle\n",
      "36 water hose sink tank python faucet plastic tube bucket siphon\n",
      "37 does job guinea product use kennel great pigs pig good\n",
      "38 cat cats toy like play love loves just toys kitten\n",
      "39 dog size small plastic kibble frisbee large harder larger unless\n",
      "40 shake christmas time make thicker does lost steel noise really\n",
      "41 door fits perfectly easy fit little plastic open just great\n",
      "42 litter box cat cats boxes clean just use cleaning like\n",
      "43 covers sells good chinchilla cool breeder just work hooks handy\n",
      "44 like just way chart size test time really color look\n",
      "45 just like love don think dogs product thing didn know\n",
      "46 kittens scratch love described great product use mess like foster\n",
      "47 doesnt great town just locally use works really supplies amazon\n",
      "48 treats training dog use treat puppy pads dogs great size\n",
      "49 plants tank sand substrate light gravel black color aquarium planted\n",
      "50 catnip use little wall smell sweater product good just quality\n",
      "51 toy loves keeps dog puppy play chew busy great kong\n",
      "52 cut nails dog nail use dogs clippers sharp cutting small\n",
      "53 crate dog puppy doors door metal divider easy room crates\n",
      "54 eyes eye doggles dog really just product use little intended\n",
      "55 water pump fountain clean cats drink just bowl quiet motor\n",
      "56 just tank like make sure surface work really tanks little\n",
      "57 like smell don liver stuff ve weird catnip dried really\n",
      "58 read good little reviews bigger size easy quality accurate works\n",
      "59 scoop bedding return time just years carefresh use amazon little\n",
      "60 fleas flea frontline dog product dogs ticks used use works\n",
      "61 collar dog barking bark spray stop dogs worked battery work\n",
      "62 replacement scratcher ich turbo used fish use catfish pads didnt\n",
      "63 baby powder vet wont giving weight dog relief days suppose\n",
      "64 dogs dog oil food salmon supplement product ingredients coat health\n",
      "65 food bowl dog container water bowls easy bag dry cat\n",
      "66 hair comb brush dog use coat great grooming tool dogs\n",
      "67 guess funny ve dog heard little bought got think thing\n",
      "68 dog collar yard time doesn house got just dogs fence\n",
      "69 tear apart wear years dog signs year old marks cosequin\n",
      "70 tank gallon clean great water fish easy works use glass\n",
      "71 pup like got purchased general talk bought little hand just\n",
      "72 mat carpet kitties floor just does kitty vacuum little floors\n",
      "73 got dog just puppy yorkie collie border small keys duty\n",
      "74 product dogs difference old vet taking noticed years pain just\n",
      "75 toys dogs toy like mr great time dog away just\n",
      "76 brush hair cat fur like cats doesn long short does\n",
      "77 price pet amazon store great product good local stores cheaper\n",
      "78 puppies 18 like got pounds problem dogs old till 10\n",
      "79 fish water love betta tank use product healthy great like\n",
      "80 cage hamster wheel hamsters inches ferrets cages little pros small\n",
      "81 kong butter peanut treats dog stuff treat dogs kongs freeze\n",
      "82 love eat lasts time long food thanks dogs friends great\n",
      "83 leash dog harness walking like just head walk pull need\n",
      "84 bottle leak just use like using does problem ve day\n",
      "85 pup puppy dog good time adult great quickly bought going\n",
      "86 water fish tank aquarium don ve gallons sponge tanks just\n",
      "87 algae plants growing tabs cat flourish use using like excel\n",
      "88 noise fry just don good rated water like great fix\n",
      "89 bulb med bulbs lamp turtle zoo clamp light heater heat\n",
      "90 chew bone chewing dog puppy nylabone bones chewed dogs chews\n",
      "91 red kong extreme boxer products dogs instantly ve black version\n",
      "92 bag carrier ferret just like bought little fit pound small\n",
      "93 fish feed pellets hay like food flakes love enjoy eat\n",
      "94 thermometer service cup customer unit suction problem just little company\n",
      "95 toy dog toys loves chew chewer dogs like size durable\n",
      "96 golden retriever dog dogs shelter animal rawhide use did expensive\n",
      "97 great littermaid traveling tape plastic works bags bought com product\n",
      "98 poop grass scooper rake just like sticks works poo ve\n",
      "99 ball balls throw dog tennis play fetch chuckit loves great\n"
     ]
    }
   ],
   "source": [
    "def print_topic(topic_term, topic_id, vocabulary):\n",
    "    print(\"{:2d} \".format(topic_id) + \" \".join(vocabulary[i] for i in topic_term[i].argsort()[:-11:-1]))\n",
    "\n",
    "for i, _  in enumerate(topic_term):\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the topics are reasonably coherent, so I'm going to move on.\n",
    "\n",
    "If things look messy:\n",
    " - n_topics might be too large, either for the diversity in the corpus (maybe there really aren't 1000 topics), or for the number of documents you have (you just don't have enough data)\n",
    " - n_topics might be too low (real topics have to be merged together by the algorithm, which doesn't work well)\n",
    " - you've got a bug!\n",
    " \n",
    "Setting n_topics very small (say 5) or very high (say 1000) is a good way of building up some intuition for what works (although beware `n_topics=1000` will take a long time to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect a document in the corpus\n",
    "\n",
    "The topic model is the lens through which we're going to view future documents.\n",
    "\n",
    "But let's first look at our existing documents through this lens.\n",
    "\n",
    "To do that we have to transform the documents we trained on to be distributions of topics (e.g. document 1 is 20% topic A, 30% topic B, etc.)\n",
    "\n",
    "We do that by running the `lda.transform` method on the vectorized documents `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_topic` has a row for each document, and a column for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, each row should add up to 1, and again they don't because of a bug in scikit-learn, so let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic /= doc_topic.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's look at the topic distribution of a random document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Although I didn't order this from Amazon, I highly highly highly recommend this product. I get mine from my local pet store (when I get a coupon) but spent a lot of time here reading reviews. I did months of research on different supplements, allergy relief, and all in one solution for my dogs health. I wanted him on Vitamin C to prevent hip problems later on, wanted something to give a healthy coat (not salmon oil) and wanted something natural and made in the USA. I can understand the ingredients and the product has been around for years. I am very pleased with it, my dogs allergies are under control, his coat his healthy and shiny, and he has never ending amounts of energy. He feels great, looks great, and acts great. When you add water it makes a great gravy for their food and he spends a lot of time licking the bowl CLEAN. Don't waste time/money/effort looking at other products, getting crap you see at Petsmart, buy NuPro and let it work. You wont be disappointed.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64 77 26 23 75]\n"
     ]
    }
   ],
   "source": [
    "top_topics = (doc_topic[1234]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 dogs dog oil food salmon supplement product ingredients coat health\n",
      "77 price pet amazon store great product good local stores cheaper\n",
      "26 size cans old fit petco petsmart amazon oz bought 13\n",
      "23 food eat brand foods brands eating tried like chicken dog\n",
      "75 toys dogs toy like mr great time dog away just\n"
     ]
    }
   ],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis is a comprehensive package for visualizing the results of a topic model. It's useful for understanding the structure of the model you've just discovered. The topics exist in a huge space. This package squeezes things down to 2D so we can look at it on the screen.\n",
    "\n",
    "In my experience, it generates a ton of spurious warnings, so let's disable warnings for this package when we import it (a useful trick!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    try:\n",
    "        import pyLDAvis\n",
    "    except ImportError:\n",
    "        print('ERROR: pyLDAvis not installed! Skip to next section!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `topic_term` and `doc_topic` matrices, pyLDAvis needs to know how often each word occurs in the entire corpus, and how long each document is. Here are calculations that give those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_frequency = np.asarray(X.sum(axis=0)).squeeze()\n",
    "doc_lengths = [len(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.prepare(topic_term_dists=topic_term,\n",
    "                           doc_topic_dists=doc_topic,\n",
    "                           doc_lengths=doc_lengths,\n",
    "                           vocab=vocabulary,\n",
    "                           term_frequency=term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all this together in a Pipeline and persist the model\n",
    "\n",
    "The process of getting from document to topic distribution is a little fiddly. We need to:\n",
    " - Vectorize the document (using the same vocabulary we used when training above)\n",
    " - Transform the document using the LDA object\n",
    " \n",
    "scikit-learn allows us to bundle these steps (and more!) together in an object called a `Pipeline`, which we can save to disk, reload, and work with again. Let's build one, train it, and save it.\n",
    "\n",
    "**WARNING**: this next cell will take a while to execute the first time you run it. After that though, the model will be loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "try:\n",
    "    with open('topic_model.pkl', 'rb') as f:\n",
    "        topic_pipeline = pickle.load(f)\n",
    "    pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()\n",
    "except IOError:\n",
    "    topic_pipeline = make_pipeline(\n",
    "        CountVectorizer(stop_words='english', max_features=10000),\n",
    "        LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "    )\n",
    "    topic_pipeline.fit(texts)\n",
    "    with open('topic_model.pkl', 'wb') as f:\n",
    "        pickle.dump(topic_pipeline, f)\n",
    "        \n",
    "pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()\n",
    "pipeline_topic_term = topic_pipeline.steps[1][1].components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine topics of a new document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single document we looks at above was a pretty short document. Let's make a more interesting, longer document out of all the reviews of that product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomreviews = \" \".join(texts[1100:1105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Careful people! Even though this chew toy says for powerful chewers, it also says for dogs up to 50 lbs in VERY tiny print on the bottom of the package. I gave it to my 79 lb Lab Mix puppy anyway, since I had already bought it... and lo and behold, it began falling apart in only a few moments.After removing the toy and picking up the white bits all over... I thought to come here and warn people. I find it odd how it's for powerful chewers yet only includes medium sized dogs in the toy description! With pit bull terriers and a few other smaller breeds as an exception... usually smaller dog means less destructive chewing ability. Senseless, really.If you have a dog that falls under this weight category... except for a breed with very powerful jaws, this toy probably suits just fine, but if you have a large dog, don't even think about it. I paid almost $9 for a toy that begun to disintegrate within a few seconds. I bought the Nylabone dinosaur chew toy for my 80lb. Greyhound and instead my 20lb Bichon has taken charge of it.  He chews on it several times a day.  The Greyhound doesn't stand a chance getting to it! I was disappointed in this one - Nylabone Durable toys are the only toys I buy for my powerful chewers because they are the most durable I've found to withstand their chewing. This one did not live up to the Nylabone Durable name. I gave it to one of my dogs, who is not the most aggressive chewer in the pack and I had to take it away from him the first day because he was able to destroy part of it and chew off pieces.  I will go back to the Nylabone Durable Souper bones from now on! It all depends on whether your dog likes it or not. My friend's dog chews up toys within a day or two of getting them. He has this one still after several months. The head is gone, but the rest is intact. My boyfriend's dog, however, only picks it up when she knows I'm waiting for her to play with it. My only complaint is that when your dog is carrying it around, if he scrapes up against you with the little nubs can be pretty sharp. We have two lab mixes. The youngest is still very much in puppy stage and boy does he love to chew! We had given him rawhides to keep his focus on something appropriate to chew, but he was going through too many rawhides. I saw nylabones in petsmart in the heavy chewers section. The first plain bone went over well. So I ordered more on Amazon. I'm so glad I did! These were the best solution to keep our chewer occupied! He loves all the different shapes and flavor toys that they have! Thank goodness for nylabones!\n"
     ]
    }
   ],
   "source": [
    "print(randomreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = topic_pipeline.transform([randomreviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "print(doc_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65 31 70 53 86]\n"
     ]
    }
   ],
   "source": [
    "top_topics = (doc_topic[0]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 chew dog toy chewer chewing toys dogs minutes chewed bone\n",
      "31 chew nylabone puppy dog dogs loves chews chewing like bones\n",
      "70 large don dog ends like size just small tend medium\n",
      "53 hair brush loose dog fur cat doesn use like does\n",
      "86 toy favorite loves dog toys cat love play cats plays\n"
     ]
    }
   ],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(pipeline_topic_term, i, pipeline_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What next?\n",
    "\n",
    "Use the `doc_topic` array for a downstream task, e.g.\n",
    " - corpus exploration (remember the [NYT visualization](http://christo.cs.umass.edu/NYT/))\n",
    " - document clustering, e.g. use something like `KMeans` (in scikit-learn) to visualize which documents are most similar in terms of their topics, which may surface groups of topics or groups of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarization\n",
    "\n",
    "Here's a short algorithm, but see [Fast Forward Labs Report 04](http://ff04.fastforwardlabs.com) for details:\n",
    "  - Train LDA on all products of a certain type (e.g. all the books)\n",
    "  - Treat all the reviews of a particular product as one document, and infer their topic distribution\n",
    "  - Infer the topic distribution for each sentence\n",
    "  - For each topic that dominates the reviews of a product, pick some sentences that are themselves dominated by that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/strain.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Be aware of limitations:\n",
    " - Choosing `n_topics` is an art rather than a science!\n",
    " - The topics don't come with names. Sometimes they overlap. Sometimes they're not what you want them to be. For example, if you run a topic model on the NYT corpus, there's no guarantee you'll get topics that correspond to the sections of the newspaper (business, metro, world, sport, etc.!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The way you used `fit` and `transform` for both the `vectorizer`, `lda`, and `topic_pipeline` objects is generic across scikit-learn, so play with scikit-learn, e.g. [Andreas Mueller's presentation](https://www.youtube.com/watch?v=8CzwlZbwDkI) is a good place to start.\n",
    " \n",
    "Remember if you're interested in the LDA algorithm itself, take a look at\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
