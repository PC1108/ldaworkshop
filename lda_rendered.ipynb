{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text\n",
    "\n",
    "There are lots of things you might want to do with documents\n",
    " - group them together\n",
    " - summarize one or several of them\n",
    " - classify them (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what they discuss\n",
    "\n",
    "To do any of these things using a computer, you first need to convert words into numbers.\n",
    "\n",
    "[https://github.com/fastforwardlabs/usaa-lda](https://github.com/fastforwardlabs/usaa-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bags of words\n",
    "\n",
    "The \"bag of words\" approach is the simplest method for this.\n",
    "\n",
    "![Bag of words](img/bagwords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are at least three problems with this approach:\n",
    "\n",
    " - **long, sparse data**: a short text is turned into a very long series of numbers, most of which are zero (\"sparse\"). The problem is not that this is wasteful (although it is that!), it's that this sparsity makes applying even simple algorithms tough. You have to apply clever or heuristic algorithms to group documents\n",
    " \n",
    " - **synonyms and multiple meanings**: The bag of words version of a document that mentions \"movies\" and a document that mentions \"films\" are different (they should not be), and a document that mentions the kind of \"bow\" that shoots an arrow is similar to one about the kind of \"bow\" you make to a King.\n",
    " \n",
    " - **word order**: totally lost (\"man bites dog\" and \"dog bites man\" are the same).\n",
    "\n",
    "We're going to look at a technique that addresses the first of these two problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling\n",
    "\n",
    "Topic modeling is a statistical method to find groups of words that tend to co-occur in a corpus of documents.\n",
    "\n",
    "For example, maybe the words \"movie\", \"film\" and \"director\" often occur in the same documents. That would make them a \"topic\".\n",
    "\n",
    "Topic modeling algorithms find these groups automatically. They are an instance of the class of algorithms known as \"unsupervised machine learning\".\n",
    "\n",
    "In doing this, we become able to express documents as a combination of a relatively small number (~100) of topics, rather than thousands of words (most of which don't occur), and we can treat documents about \"films\" and \"movies\" similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling workflow\n",
    "\n",
    "Topic modeling has two steps:\n",
    "\n",
    " - learn topics from a corpus of representative documents\n",
    " - figure out which of these topics occur in the particular document(s) you're interested in\n",
    " \n",
    "If you're a machine learning person, you'll recognize these as training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_topics.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_evaluate.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once you've done the second step, you've expressed your new document as a short vector of numbers that you can no do all sorts of things with:\n",
    "\n",
    " - group documents together\n",
    " - summarize one or several documents\n",
    " - classify it (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what people are discussing, e.g. [Time-series plots of 1000 topics extracted from 20 years of the New York Times.](http://christo.cs.umass.edu/NYT/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "The best known and best algorithm for finding topics in a corpus is Latent Dirichlet Allocation. It's got a complicated name and, to be frank, it's a complicated algorithm. If you'd like to begin to dig into the details, there are two resources I recommend very highly!\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
    " \n",
    "For the purposes of this talk, I'm just going to say it finds groups of words that co-occur by magic!\n",
    "\n",
    "The good news is, there are several excellent open source implementations of the algorithm, and we're going to use one of those today.\n",
    "\n",
    "We're going to apply it to a public dataset of Amazon product reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Load data\n",
    "\n",
    "The cell below opens the file and loads it into a pandas dataframe.\n",
    "\n",
    "There's a lot going on in this line, all of which is useful to understand if you're a Python programmer, but none of which is necessary to understand if you're only interested in LDA.\n",
    "\n",
    "If you would like a more detailed explanation of what's going on, please see [the Data notebook](data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"reviews.json\", 'rb') as f:\n",
    "    reviews = pd.DataFrame(json.loads(l.decode()) for l in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas dataframe is a structured table-like object that, among many other things, supports a bunch of SQL-like operations and handles fiddly data types like data and times well. I don't know much about R, but I understand R has objects like this too.\n",
    "\n",
    "`head` allows us to see the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I purchased the Trilogy with hoping my two cat...</td>\n",
       "      <td>01 12, 2011</td>\n",
       "      <td>A14CK12J7C7JRK</td>\n",
       "      <td>Consumer in NorCal</td>\n",
       "      <td>Nice Distraction for my cats for about 15 minutes</td>\n",
       "      <td>1294790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>There are usually one or more of my cats watch...</td>\n",
       "      <td>09 14, 2013</td>\n",
       "      <td>A39QHP5WLON5HV</td>\n",
       "      <td>Melodee Placial</td>\n",
       "      <td>Entertaining for my cats</td>\n",
       "      <td>1379116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I bought the triliogy and have tested out all ...</td>\n",
       "      <td>12 19, 2012</td>\n",
       "      <td>A2CR37UY3VR7BN</td>\n",
       "      <td>Michelle Ashbery</td>\n",
       "      <td>Entertaining</td>\n",
       "      <td>1355875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>My female kitty could care less about these vi...</td>\n",
       "      <td>05 12, 2011</td>\n",
       "      <td>A2A4COGL9VW2HY</td>\n",
       "      <td>Michelle P</td>\n",
       "      <td>Happy to have them</td>\n",
       "      <td>1305158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>If I had gotten just volume two, I would have ...</td>\n",
       "      <td>03 5, 2012</td>\n",
       "      <td>A2UBQA85NIGLHA</td>\n",
       "      <td>Tim  Isenhour \"Timbo\"</td>\n",
       "      <td>You really only need vol 2</td>\n",
       "      <td>1330905600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  1223000893  [0, 0]      3.0   \n",
       "1  1223000893  [0, 0]      5.0   \n",
       "2  1223000893  [0, 0]      4.0   \n",
       "3  1223000893  [2, 2]      4.0   \n",
       "4  1223000893  [6, 7]      3.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I purchased the Trilogy with hoping my two cat...  01 12, 2011   \n",
       "1  There are usually one or more of my cats watch...  09 14, 2013   \n",
       "2  I bought the triliogy and have tested out all ...  12 19, 2012   \n",
       "3  My female kitty could care less about these vi...  05 12, 2011   \n",
       "4  If I had gotten just volume two, I would have ...   03 5, 2012   \n",
       "\n",
       "       reviewerID           reviewerName  \\\n",
       "0  A14CK12J7C7JRK     Consumer in NorCal   \n",
       "1  A39QHP5WLON5HV        Melodee Placial   \n",
       "2  A2CR37UY3VR7BN       Michelle Ashbery   \n",
       "3  A2A4COGL9VW2HY             Michelle P   \n",
       "4  A2UBQA85NIGLHA  Tim  Isenhour \"Timbo\"   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0  Nice Distraction for my cats for about 15 minutes      1294790400  \n",
       "1                           Entertaining for my cats      1379116800  \n",
       "2                                       Entertaining      1355875200  \n",
       "3                                 Happy to have them      1305158400  \n",
       "4                         You really only need vol 2      1330905600  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual columns can be accessed as keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1223000893\n",
       "1    1223000893\n",
       "2    1223000893\n",
       "3    1223000893\n",
       "4    1223000893\n",
       "Name: asin, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['asin'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 reviews\n",
      "of 8019 products \n",
      "by 19834 unique authors \n"
     ]
    }
   ],
   "source": [
    "print(\"{} reviews\".format(len(reviews)))\n",
    "print(\"of {} products \".format(len(reviews.asin.unique())))\n",
    "print(\"by {} unique authors \".format(len(reviews.reviewerID.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize reviews\n",
    "\n",
    "You have to preprocess the text a little before you apply LDA: you need to split documents into words, and you need to turn words into vectorized numbers.\n",
    "\n",
    "Ironically, in order to get the benefits of LDA, we first need to run bag of words on our data!\n",
    "\n",
    "The code to do this is built into scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True, max_features=10000)\n",
    "X = vectorizer.fit_transform(reviews['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.18 s, sys: 3.91 s, total: 6.09 s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "lda.fit(X[:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you should do when you fit a topic model is inspect a few of the words that dominate each topic to check that the topics are coherent.\n",
    "\n",
    "To do this, we need to look at the `components_` attribute now attached to `lda`. This is an array with `n_topics` rows and a number of columns equal to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in this array is the weight of the corresponding word in the corresponding topic.\n",
    "\n",
    "The weights of each topic should add up to one, i.e. each row of `lda.components_` should add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3408.92129467,   3102.9078119 ,   6336.39414052,  11663.78298759,\n",
       "         6490.58482843,   2169.81552908,   8859.48340892,   4534.03122498,\n",
       "         5448.7790058 ,   5755.0881906 ,   4889.51340858,   5075.79388066,\n",
       "         8843.02416106,   7355.26193235,   9079.94863263,   5157.55260889,\n",
       "         5280.97273017,   4702.69994325,   2572.84577759,   4126.57521393,\n",
       "         4053.29010868,   5431.65181866,   2312.3476769 ,   3141.97971945,\n",
       "         3274.40201268,   2722.35552177,  10258.55265971,   2627.74462264,\n",
       "         3631.71145661,   3163.75557003,   5398.16158546,   3139.12988856,\n",
       "         3338.19691485,   3291.67852031,  12333.21738803,  11198.25555185,\n",
       "         2398.44318405,   2716.57729782,   7590.03002085,   4553.21892308,\n",
       "         6128.35697499,   3245.63827339,   2892.78490628,   2806.79505779,\n",
       "         2739.62518746,   2296.39971362,   2874.21059793,   3977.09070756,\n",
       "         3425.73783267,   3456.18167458,   3552.11374289,  12284.0741342 ,\n",
       "         2782.1136586 ,   4857.03140007,   6792.55548935,   5465.29916879,\n",
       "         3116.26718938,   4079.41714736,  23240.21606213,   2665.92383332,\n",
       "         3441.6052531 ,   3434.89568975,  12171.47351485,   5703.67538531,\n",
       "         9190.69121894,   3047.67587678,   3001.25552383,   2683.88457955,\n",
       "         2742.63562575,   5398.16480851,   3592.69835061,   6598.10619643,\n",
       "         3337.15227257,   4137.52385664,   9914.57856659,   3413.26531751,\n",
       "         3992.47731737,   4588.44673542,   5145.41945699,   4551.20436576,\n",
       "         3825.47858313,  28655.18241237,   2716.21263485,   5269.87497696,\n",
       "         3197.34356397,   3596.82731559,   3476.24709767,   5925.16297384,\n",
       "        23302.28644016,   4403.40768666,   3615.94418309,   4905.73587673,\n",
       "         3530.47758848,   8415.51912165,  15334.7558847 ,  17225.66725565,\n",
       "         2725.77233757,   5316.52180812,   5864.80523827,   4021.43720194])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear! It turns out there's a bug in scikit-learn's implementation of LDA. Let's fix it here. This should be fixed in the next version of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array of topics and words (or terms) is usually called the topic-term matrix, so let's save it under that name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_term = lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word corresponding to each column in this array, which we'll call the `vocabulary`, is available as a list from `vectorizer.get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', 'bits', 'contest', 'emits', 'grid', 'leading', 'opt', 'really', 'slice', 'tightening']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(vocabulary[:10000:1000]) # print the 0th, 999th, 1999th, 2999th, etc. item in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the top 10 words that dominate each topic.\n",
    "\n",
    "It does that by going through each row (i.e. each topic) in the `topic_term` array, finding the biggest numbers in that row, then finding the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 good product used airline use just tubing like basic 25\n",
      " 1 okay hamster thank just dwarf hamsters like don use area\n",
      " 2 water fish tank stress conditioner change product use tap coat\n",
      " 3 price store amazon pet local cheaper stores buy product buying\n",
      " 4 test kit water accurate ammonia easy use testing tests ph\n",
      " 5 packaged bunnies arrived hedgehog heavy easy use bought gerbils duty\n",
      " 6 fit cage fits easy perfectly nice great room size bought\n",
      " 7 dried worms use liver freeze easy pills make like product\n",
      " 8 really like just product think don time use sure did\n",
      " 9 apart tough toys stuffing rip ripped squeaker tear dog toy\n",
      "10 like 34 dog don make just use great easy works\n",
      "11 turtle bags tank use just seachem purigen filter bulb product\n",
      "12 feeder set food time cat just 12 review cats hours\n",
      "13 works handle use great grass poop tool job dog good\n",
      "14 food container bag dog holds open great lid just easy\n",
      "15 don didn know dogs doesn away bought ingredient won dog\n",
      "16 directions followed instructions bubble follow like went just doing days\n",
      "17 cute loves dog guy toy little golden retriever play sound\n",
      "18 cat date really just ribbon appears good make better reviews\n",
      "19 puppies kennel love jack kitties terrier russell choking time hard\n",
      "20 guinea cage priced hay rabbit pigs bedding pig like bunny\n",
      "21 water use great fish prime product tank best price changes\n",
      "22 like thing consistently little make doesn finished ick works know\n",
      "23 perch purpose perches time serves great washed use don like\n",
      "24 breed doberman 50 puppy far dog like time likes hopefully\n",
      "25 just leash like dog use breeds works flexi don hold\n",
      "26 food fish eat love feed like feeding pellets tank water\n",
      "27 like time does really bad tried rated just work don\n",
      "28 using product koi shiny long just love time great coat\n",
      "29 just launcher calcium use vitamin 12 source max make supposed\n",
      "30 make carry makes noise jolly small dog ball large holes\n",
      "31 inches like inch bit work don make right high used\n",
      "32 great picks just new cracked seller time shipment arrived heavy\n",
      "33 awesome great med lamp doesnt similar heat package just zoo\n",
      "34 treats kong dog butter peanut treat toy puppy dogs stuff\n",
      "35 tank water gallon fish clear use great works filter clean\n",
      "36 kinda just chocolate use like cheaply thought got ignored didn\n",
      "37 summer lazy sheds impressed like dogs little grab dog winter\n",
      "38 flea fleas frontline product used dogs vet ticks years use\n",
      "39 handy size leash large walking medium small dog works great\n",
      "40 spray chewing dog puppy taste smell sprayed does works work\n",
      "41 40 60 like handles adjust convenient little seals buy really\n",
      "42 use food best need time dogs good dog crickets work\n",
      "43 lock used work surprise cheap plastic outer cockatiels hurting did\n",
      "44 expecting good clipper like really ease bought quickly did smaller\n",
      "45 cichlid bonus like cichlids water african waterer 75 longer use\n",
      "46 enjoyed paid really small brother son instantly time dogs entertained\n",
      "47 crazy cat catnip cats cardboard thing couch just really scratch\n",
      "48 cats cat love replacement scratcher future turbo reasonable write scratching\n",
      "49 switched dog using alot fleas tried dogs frontline works use\n",
      "50 just product say little recommend work time need like veggies\n",
      "51 food dog cat good eat like diet health dogs ingredients\n",
      "52 gift just dog got think ve like disappointed hard little\n",
      "53 just ve use make times sure like tried don does\n",
      "54 pit bull loves minutes dog treats day busy good puppy\n",
      "55 cut nails sharp nail dog cutting quick clippers use easy\n",
      "56 miracle nature product great just recommend multiple red urine area\n",
      "57 trained train use just took think training doesn bit don\n",
      "58 toy dog chew toys loves chewer chewing puppy dogs minutes\n",
      "59 bringing years buckets holder peace thing home man does 18\n",
      "60 softer bright brown floors rats color like wood dog easy\n",
      "61 broke turtles sent product turtle better aquatic quality time great\n",
      "62 tank plants water fish aquarium use algae just planted tanks\n",
      "63 price great product fast good baby recommend shipping works love\n",
      "64 cat cats toy play love like toys kitten kittens loves\n",
      "65 blue color pink got like blood light did just great\n",
      "66 time just little years manage thing does right buy old\n",
      "67 farther use old just like readily great product day years\n",
      "68 day time consistent just like bought easily little wooden quite\n",
      "69 dog dogs great nails german shepherd nail like puppy use\n",
      "70 just training barks product basically quickly got think like doesn\n",
      "71 skin oil coat dog salmon dogs product coats like just\n",
      "72 bear like don dog thing pay good know use teddy\n",
      "73 item return received ordered colors amazon product send company color\n",
      "74 hair brush fur cat does use dog short doesn like\n",
      "75 little small product like don work wore realize didn hard\n",
      "76 time just watch like bored really little ll make large\n",
      "77 drink cats product vet cat just suggested stomach pet water\n",
      "78 birds bird cage parrot seed love seeds parakeet treat day\n",
      "79 like far reviews don said read just stars product know\n",
      "80 just cartridges filter bag water new tank does think great\n",
      "81 litter box cat cats clean just use boxes like scoop\n",
      "82 everyday dishwasher sweet use easy night turn dog clean does\n",
      "83 smell smells like clean product shampoo dog smelling scent use\n",
      "84 shorter like time just simply long little use mechanism want\n",
      "85 useful remove use like charm works easily save cleaning feel\n",
      "86 like product just rating bit combined new long make didn\n",
      "87 bowl water clean bowls cats product use cat easy odors\n",
      "88 filter filters water tank media carbon flow clean just quiet\n",
      "89 stain carpet works highly accidents stains recommend pet use odor\n",
      "90 touch edges dog distance retrieve little good far love heard\n",
      "91 keeps warm busy nice heat hot bag stay cold night\n",
      "92 did reviews didn awful knew clipped product don use read\n",
      "93 dog collar dogs stopped barking bark time works product work\n",
      "94 chew dogs dog bone love nylabone toy lab recommend loves\n",
      "95 ball toy play dog loves playing balls throw toys dogs\n",
      "96 hit miss just year cat stars did noticeable bought sure\n",
      "97 teeth dog toothpaste flavor like dogs brush breath loves taste\n",
      "98 crate dog sturdy size puppy easy allows crates doors car\n",
      "99 fun bounces christmas dogs love dog toy way shaped good\n"
     ]
    }
   ],
   "source": [
    "def print_topic(topic_term, topic_id, vocabulary):\n",
    "    print(\"{:2d} \".format(topic_id) + \" \".join(vocabulary[i] for i in topic_term[i].argsort()[:-11:-1]))\n",
    "\n",
    "for i, _  in enumerate(topic_term):\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the topics are reasonably coherent, so I'm going to move on.\n",
    "\n",
    "If things look messy:\n",
    " - n_topics might be too large, either for the diversity in the corpus (maybe there really aren't 1000 topics), or for the number of documents you have (you just don't have enough data)\n",
    " - n_topics might be too low (real topics have to be merged together by the algorithm, which doesn't work well)\n",
    " - you've got a bug!\n",
    " \n",
    "Setting n_topics very small (say 5) or very high (say 1000) is a good way of building up some intuition for what works (although beware `n_topics=1000` will take a long time to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect a document in the corpus\n",
    "\n",
    "The topic model is the lens through which we're going to view future documents.\n",
    "\n",
    "But let's first look at our existing documents through this lens.\n",
    "\n",
    "To do that we have to transform the documents we trained on to be distributions of topics (e.g. document 1 is 20% topic A, 30% topic B, etc.)\n",
    "\n",
    "We do that by running the `lda.transform` method on the vectorized documents `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_topic` has a row for each document, and a column for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, each row should add up to 1, and again they don't because of a bug in scikit-learn, so let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic /= doc_topic.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's look at the topic distribution of a random document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My toy poodle loves this stuff and will let me &#34;sort&#34; of brush her teeth because of it.  I was hoping it would help with her doggy breath and it does some.  Interestingly... it says &#34;peanutbutter&#34; but it doesn't smell like peanutbutter.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.loc[7, 'reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97 10 91 36 26]\n"
     ]
    }
   ],
   "source": [
    "top_topics = (doc_topic[7]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 teeth dog toothpaste flavor like dogs brush breath loves taste\n",
      "10 like 34 dog don make just use great easy works\n",
      "91 keeps warm busy nice heat hot bag stay cold night\n",
      "36 kinda just chocolate use like cheaply thought got ignored didn\n",
      "26 food fish eat love feed like feeding pellets tank water\n"
     ]
    }
   ],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis is a comprehensive package for visualizing the results of a topic model. It's useful for understanding the structure of the model you've just discovered. The topics exist in a huge space. This package squeezes things down to 2D so we can look at it on the screen.\n",
    "\n",
    "In my experience, it generates a ton of spurious warnings, so let's disable warnings for this package when we import it (a useful trick!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    try:\n",
    "        import pyLDAvis\n",
    "    except ImportError:\n",
    "        print('ERROR: pyLDAvis not installed! Skip to next section!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `topic_term` and `doc_topic` matrices, pyLDAvis needs to know how often each word occurs in the entire corpus, and how long each document is. Here are calculations that give those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_frequency = np.asarray(X.sum(axis=0)).squeeze()\n",
    "doc_lengths = reviews['reviewText'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.prepare(topic_term_dists=topic_term,\n",
    "                           doc_topic_dists=doc_topic,\n",
    "                           doc_lengths=doc_lengths,\n",
    "                           vocab=vocabulary,\n",
    "                           term_frequency=term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all this together in a Pipeline and persist the model\n",
    "\n",
    "The process of getting from document to topic distribution is a little fiddly. We need to:\n",
    " - Vectorize the document (using the same vocabulary we used when training above)\n",
    " - Transform the document using the LDA object\n",
    " \n",
    "scikit-learn allows us to bundle these steps (and more!) together in an object called a `Pipeline`, which we can save to disk, reload, and work with again. Let's build one, train it, and save it.\n",
    "\n",
    "**WARNING**: this next cell will take a while to execute the first time you run it. After that though, the model will be loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "try:\n",
    "    with open('topic_model.pkl', 'rb') as f:\n",
    "        topic_pipeline = pickle.load(f)\n",
    "    pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()\n",
    "except IOError:\n",
    "    topic_pipeline = make_pipeline(\n",
    "        CountVectorizer(stop_words='english', binary=True, max_features=10000),\n",
    "        LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "    )\n",
    "    topic_pipeline.fit(reviews.loc[:20000, 'reviewText'])\n",
    "    with open('topic_model.pkl', 'wb') as f:\n",
    "        pickle.dump(topic_pipeline, f)\n",
    "        \n",
    "pipeline_topic_term = topic_pipeline.steps[1][1].components_\n",
    "pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine topics of a new document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single document we looks at above was a pretty short document. Let's make a more interesting, longer document out of all the reviews of that product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    My Rottie has food allergies to poultry, beef ...\n",
       "6    My puppy loves this stuff! His tail starts wag...\n",
       "7    My toy poodle loves this stuff and will let me...\n",
       "8    Works great and dog doesn't hate the taste.  G...\n",
       "9    Yes , my Princess is enjoying the taste showin...\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.query('asin == \"4847676011\"')['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews4847676011 = reviews.query('asin == \"4847676011\"')['reviewText'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Rottie has food allergies to poultry, beef and dairy. I've had a difficult time finding a toothpaste that doesn't make him allergic and he enjoys the taste. This toothpaste is peanut flavor (smells like black licorice). He loves the taste and doesn't wiggle as much when I brush his teeth every night. The price is ok, but I do wish that the tube came in a larger size. Soooo, if your pup has allergies or doesn't like his/her current toothpaste you might want to try this one. My puppy loves this stuff! His tail starts wagging as soon as I ask him if he's ready to brush his teeth! It is actually an enjoyable daily experience! Definitely my &#34;Go To&#34; dog toothpaste. My toy poodle loves this stuff and will let me &#34;sort&#34; of brush her teeth because of it.  I was hoping it would help with her doggy breath and it does some.  Interestingly... it says &#34;peanutbutter&#34; but it doesn't smell like peanutbutter. Works great and dog doesn't hate the taste.  Gum health is important so just have to brush those pearly whites. Yes , my Princess is enjoying the taste showing that She is getting best/top results .. She loves that and me too ... Strongly recommended , without regret ...\n"
     ]
    }
   ],
   "source": [
    "print(reviews4847676011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = topic_pipeline.transform([reviews4847676011])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "print(doc_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51  2 57 84 66]\n"
     ]
    }
   ],
   "source": [
    "top_topics = (doc_topic[0]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 teeth dog dogs taste like flavor toothpaste brush loves breath\n",
      " 2 trimmed midwest cool just crates gobble don spending like got\n",
      "57 treats dog dogs treat extreme pieces small just don kong\n",
      "84 dog skin coat dogs oil soft food product coats dry\n",
      "66 fight love round ve make time great residue sure biggest\n"
     ]
    }
   ],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(pipeline_topic_term, i, pipeline_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What next?\n",
    "\n",
    "Use the `doc_topic` array for a downstream task, e.g.\n",
    " - corpus exploration (remember the [NYT visualization](http://christo.cs.umass.edu/NYT/))\n",
    " - document clustering, e.g. use something like `KMeans` (in scikit-learn) to visualize which documents are most similar in terms of their topics, which may surface groups of topics or groups of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarization\n",
    "\n",
    "Here's a short algorithm, but see [Fast Forward Labs Report 04](http://ff04.fastforwardlabs.com) for details:\n",
    "  - Train LDA on all products of a certain type (e.g. all the books)\n",
    "  - Treat all the reviews of a particular product as one document, and infer their topic distribution\n",
    "  - Infer the topic distribution for each sentence\n",
    "  - For each topic that dominates the reviews of a product, pick some sentences that are themselves dominated by that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Be aware of limitations:\n",
    " - Choosing `n_topics` is an art rather than a science!\n",
    " - The topics don't come with names. Sometimes they overlap. Sometimes they're not what you want them to be. For example, if you run a topic model on the NYT corpus, there's no guarantee you'll get topics that correspond to the sections of the newspaper (business, metro, world, sport, etc.!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The way you used `fit` and `transform` for both the `vectorizer`, `lda`, and `topic_pipeline` objects is generic across scikit-learn, so play with scikit-learn, e.g. [Andreas Mueller's presentation](https://www.youtube.com/watch?v=8CzwlZbwDkI) is a good place to start.\n",
    " \n",
    "Remember if you're interested in the LDA algorithm itself, take a look at\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
