{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text\n",
    "\n",
    "There are lots of things you might want to do with documents\n",
    " - group them together\n",
    " - summarize one or several of them\n",
    " - classify them (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what they discuss\n",
    "\n",
    "To do any of these things using a computer, you first need to convert words into numbers.\n",
    "\n",
    "[https://github.com/fastforwardlabs/usaa-lda](https://github.com/fastforwardlabs/usaa-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bags of words\n",
    "\n",
    "The \"bag of words\" approach is the simplest method for this.\n",
    "\n",
    "![Bag of words](img/bagwords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are at least three problems with this approach:\n",
    "\n",
    " - **long, sparse data**: a short text is turned into a very long series of numbers, most of which are zero (\"sparse\"). The problem is not that this is wasteful (although it is that!), it's that this sparsity makes applying even simple algorithms tough. You have to apply clever or heuristic algorithms to group documents\n",
    " \n",
    " - **synonyms and multiple meanings**: The bag of words version of a document that mentions \"movies\" and a document that mentions \"films\" are different (they should not be), and a document that mentions the kind of \"bow\" that shoots an arrow is similar to one about the kind of \"bow\" you make to a King.\n",
    " \n",
    " - **word order**: totally lost (\"man bites dog\" and \"dog bites man\" are the same).\n",
    "\n",
    "We're going to look at a technique that addresses the first of these two problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling\n",
    "\n",
    "Topic modeling is a statistical method to find groups of words that tend to co-occur in a corpus of documents.\n",
    "\n",
    "For example, maybe the words \"movie\", \"film\" and \"director\" often occur in the same documents. That would make them a \"topic\".\n",
    "\n",
    "Topic modeling algorithms find these groups automatically. They are an instance of the class of algorithms known as \"unsupervised machine learning\".\n",
    "\n",
    "In doing this, we become able to express documents as a combination of a relatively small number (~100) of topics, rather than thousands of words (most of which don't occur), and we can treat documents about \"films\" and \"movies\" similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic modeling workflow\n",
    "\n",
    "Topic modeling has two steps:\n",
    "\n",
    " - learn topics from a corpus of representative documents\n",
    " - figure out which of these topics occur in the particular document(s) you're interested in\n",
    " \n",
    "If you're a machine learning person, you'll recognize these as training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_topics.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lda_evaluate.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once you've done the second step, you've expressed your new document as a short vector of numbers that you can no do all sorts of things with:\n",
    "\n",
    " - group documents together\n",
    " - summarize one or several documents\n",
    " - classify it (e.g. sentiment, tagging)\n",
    " - explore, e.g. temporal trends in what people are discussing, e.g. [Time-series plots of 1000 topics extracted from 20 years of the New York Times.](http://christo.cs.umass.edu/NYT/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "The best known and best algorithm for finding topics in a corpus is Latent Dirichlet Allocation. It's got a complicated name and, to be frank, it's a complicated algorithm. If you'd like to begin to dig into the details, there are two resources I recommend very highly!\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
    " \n",
    "For the purposes of this talk, I'm just going to say it finds groups of words that co-occur by magic!\n",
    "\n",
    "The good news is, there are several excellent open source implementations of the algorithm, and we're going to use one of those today.\n",
    "\n",
    "We're going to apply it to a public dataset of Amazon product reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Load data\n",
    "\n",
    "The cell below opens the file and loads it into a pandas dataframe.\n",
    "\n",
    "There's a lot going on in this line, all of which is useful to understand if you're a Python programmer, but none of which is necessary to understand if you're only interested in LDA.\n",
    "\n",
    "If you would like a more detailed explanation of what's going on, please see [the Data notebook](data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"reviews.json\", 'rb') as f:\n",
    "    reviews = pd.DataFrame(json.loads(l.decode()) for l in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas dataframe is a structured table-like object that, among many other things, supports a bunch of SQL-like operations and handles fiddly data types like data and times well. I don't know much about R, but I understand R has objects like this too.\n",
    "\n",
    "`head` allows us to see the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I purchased the Trilogy with hoping my two cat...</td>\n",
       "      <td>01 12, 2011</td>\n",
       "      <td>A14CK12J7C7JRK</td>\n",
       "      <td>Consumer in NorCal</td>\n",
       "      <td>Nice Distraction for my cats for about 15 minutes</td>\n",
       "      <td>1294790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>There are usually one or more of my cats watch...</td>\n",
       "      <td>09 14, 2013</td>\n",
       "      <td>A39QHP5WLON5HV</td>\n",
       "      <td>Melodee Placial</td>\n",
       "      <td>Entertaining for my cats</td>\n",
       "      <td>1379116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I bought the triliogy and have tested out all ...</td>\n",
       "      <td>12 19, 2012</td>\n",
       "      <td>A2CR37UY3VR7BN</td>\n",
       "      <td>Michelle Ashbery</td>\n",
       "      <td>Entertaining</td>\n",
       "      <td>1355875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>My female kitty could care less about these vi...</td>\n",
       "      <td>05 12, 2011</td>\n",
       "      <td>A2A4COGL9VW2HY</td>\n",
       "      <td>Michelle P</td>\n",
       "      <td>Happy to have them</td>\n",
       "      <td>1305158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1223000893</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>If I had gotten just volume two, I would have ...</td>\n",
       "      <td>03 5, 2012</td>\n",
       "      <td>A2UBQA85NIGLHA</td>\n",
       "      <td>Tim  Isenhour \"Timbo\"</td>\n",
       "      <td>You really only need vol 2</td>\n",
       "      <td>1330905600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  1223000893  [0, 0]      3.0   \n",
       "1  1223000893  [0, 0]      5.0   \n",
       "2  1223000893  [0, 0]      4.0   \n",
       "3  1223000893  [2, 2]      4.0   \n",
       "4  1223000893  [6, 7]      3.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I purchased the Trilogy with hoping my two cat...  01 12, 2011   \n",
       "1  There are usually one or more of my cats watch...  09 14, 2013   \n",
       "2  I bought the triliogy and have tested out all ...  12 19, 2012   \n",
       "3  My female kitty could care less about these vi...  05 12, 2011   \n",
       "4  If I had gotten just volume two, I would have ...   03 5, 2012   \n",
       "\n",
       "       reviewerID           reviewerName  \\\n",
       "0  A14CK12J7C7JRK     Consumer in NorCal   \n",
       "1  A39QHP5WLON5HV        Melodee Placial   \n",
       "2  A2CR37UY3VR7BN       Michelle Ashbery   \n",
       "3  A2A4COGL9VW2HY             Michelle P   \n",
       "4  A2UBQA85NIGLHA  Tim  Isenhour \"Timbo\"   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0  Nice Distraction for my cats for about 15 minutes      1294790400  \n",
       "1                           Entertaining for my cats      1379116800  \n",
       "2                                       Entertaining      1355875200  \n",
       "3                                 Happy to have them      1305158400  \n",
       "4                         You really only need vol 2      1330905600  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual columns can be accessed as keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1223000893\n",
       "1    1223000893\n",
       "2    1223000893\n",
       "3    1223000893\n",
       "4    1223000893\n",
       "Name: asin, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['asin'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 reviews\n",
      "of 8019 products \n",
      "by 19834 unique authors \n"
     ]
    }
   ],
   "source": [
    "print(\"{} reviews\".format(len(reviews)))\n",
    "print(\"of {} products \".format(len(reviews.asin.unique())))\n",
    "print(\"by {} unique authors \".format(len(reviews.reviewerID.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = reviews.loc[:19999, 'reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize reviews\n",
    "\n",
    "You have to preprocess the text a little before you apply LDA: you need to split documents into words, and you need to turn words into vectorized numbers.\n",
    "\n",
    "Ironically, in order to get the benefits of LDA, we first need to run bag of words on our data!\n",
    "\n",
    "The code to do this is built into scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True, max_features=10000)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 s, sys: 3.6 s, total: 5.76 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you should do when you fit a topic model is inspect a few of the words that dominate each topic to check that the topics are coherent.\n",
    "\n",
    "To do this, we need to look at the `components_` attribute now attached to `lda`. This is an array with `n_topics` rows and a number of columns equal to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in this array is the weight of the corresponding word in the corresponding topic.\n",
    "\n",
    "The weights of each topic should add up to one, i.e. each row of `lda.components_` should add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2899.94396743,   3331.25507096,   2735.92831357,   2808.3801431 ,\n",
       "         5807.76934833,  10160.34266672,   1873.57948541,   2941.4440404 ,\n",
       "         8205.30635865,   2290.37851904,   3079.69103073,   8994.70193255,\n",
       "         4089.18827416,   3321.73509238,   3985.69321464,   3009.43803248,\n",
       "         4125.98791814,   3335.25855782,  28945.50015192,   5617.2479667 ,\n",
       "         4168.09737088,   8958.82828126,   2798.76524992,   2923.27168087,\n",
       "         3387.34614447,   2503.33483141,   2642.0828188 ,   7411.09344249,\n",
       "         3738.44160854,   3298.65512336,   3832.94500296,   9573.17002722,\n",
       "         2874.72839483,   2572.09258089,   2899.28683618,  10000.06905565,\n",
       "         6104.61893954,   3537.84936702,   3481.09973182,   4843.82328102,\n",
       "         2406.07716706,  12695.78846148,   7496.16299411,   3216.43479767,\n",
       "         2660.51822386,   9271.30717504,   5733.10948604,  21434.01707315,\n",
       "         2940.66779599,   4703.39785823,   2810.21940582,   6979.79780731,\n",
       "         3821.57646031,   7776.65104959,   4299.71849201,   6164.13247197,\n",
       "         6910.27373922,   6886.52300957,   3761.90256274,   7848.25039149,\n",
       "         7478.05756503,   2888.59499097,   3176.6209351 ,   5913.89613809,\n",
       "         5080.53392331,  23598.26115645,   3131.24966179,   3376.26294885,\n",
       "         2665.90468545,   8543.52073787,   5634.74498667,   3642.26397945,\n",
       "         4154.96993075,   4249.48144811,   5809.59385664,   3586.692544  ,\n",
       "         5908.31202856,   3650.2278735 ,   3113.12382908,   3620.33290818,\n",
       "         2876.90001395,  31330.33919845,  13030.37665135,   4385.25883091,\n",
       "         7584.53297679,   6283.63244773,   6390.92008358,   3092.47686641,\n",
       "         3686.29144838,   8269.16528839,   2841.35628403,   3320.83636173,\n",
       "         5116.62667966,   2928.91584237,   3274.37152202,   5008.17766561,\n",
       "         5001.6913705 ,  11321.56931562,   3015.93370817,   2979.08503918])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear! It turns out there's a bug in scikit-learn's implementation of LDA. Let's fix it here. This should be fixed in the next version of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array of topics and words (or terms) is usually called the topic-term matrix, so let's save it under that name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_term = lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word corresponding to each column in this array, which we'll call the `vocabulary`, is available as a list from `vectorizer.get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', 'bin', 'continents', 'elsey', 'growled', 'leaning', 'oops', 'rawhide', 'sizes', 'thosefishgeeks']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(vocabulary[:10000:1000]) # print the 0th, 999th, 1999th, 2999th, etc. item in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the top 10 words that dominate each topic.\n",
    "\n",
    "It does that by going through each row (i.e. each topic) in the `topic_term` array, finding the biggest numbers in that row, then finding the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 dont shih tzu doesnt product delivered like good thing love\n",
      " 1 product brought dog great trainer work ways use home recommended\n",
      " 2 trimmed midwest cool just crates gobble don like got tastes\n",
      " 3 temp just like flat little stronger things good used cats\n",
      " 4 great product fast value good thanks price love thank recommend\n",
      " 5 treats kong butter peanut dog treat loves busy keeps toy\n",
      " 6 regularly sells terrific good product use share hey little drives\n",
      " 7 use just don bought product like want works great penguin\n",
      " 8 crate size dog puppy perfect fits sturdy easy door small\n",
      " 9 tired constructed toss like product ignore great sure da groomed\n",
      "10 good chuck favorites wont just love need em breed dog\n",
      "11 cut nails nail dog sharp quick use easy clippers work\n",
      "12 cat just maine coon like sure really dog way product\n",
      "13 ears ear like fluffy love day dog comes ve time\n",
      "14 product using don just years use works spot used smell\n",
      "15 just serves need great purpose works good right purchased use\n",
      "16 bedding rabbit wood bought paper little like bag just really\n",
      "17 good airline use nice like little just price alternative standard\n",
      "18 toy play dog toys loves ball playing dogs love fun\n",
      "19 tank plants tanks algae use great works fish product turtle\n",
      "20 chewing spray furniture sprayed dog work puppy bitter apple works\n",
      "21 test kit water easy use aquarium accurate testing tests need\n",
      "22 good product use great stable day tasty pups old dog\n",
      "23 like product great sure bought guessing just used work applied\n",
      "24 cage just hamster place won don sleep getting everyday door\n",
      "25 mistake colorful product amazed ive using love paying time buy\n",
      "26 good texture yellow love touch use product better great feel\n",
      "27 odor litter does works use box clean job good smell\n",
      "28 guinea priced pigs pig reasonably hay love cage use like\n",
      "29 got rinsed good just sure right don think rottweiler like\n",
      "30 cat cats like just milk catnip love come new really\n",
      "31 chew nylabone puppy dog dogs loves chews chewing like bones\n",
      "32 golden retriever fantastic loves wish nubs bunny like charm heads\n",
      "33 use using ve like product works dull easy comes easily\n",
      "34 cat just got ve like coming gets night really make\n",
      "35 filter filters water clean tank media carbon easy replace great\n",
      "36 time just return know think returned tried like don used\n",
      "37 expected excited feeder like weird small thats use cats way\n",
      "38 like sounds spaniel dog cocker think little funny just words\n",
      "39 light heat lamp temperature just bulb plastic tank tape great\n",
      "40 foster use stone vacuum combo absorbent size time want number\n",
      "41 price store pet amazon local cheaper stores buy product good\n",
      "42 collar bark barking dog battery batteries collars time shock worked\n",
      "43 great beat price gold food love rat dog recommend good\n",
      "44 says product use does wrong killed good tearing alive time\n",
      "45 hair brush comb works use long fur cat does like\n",
      "46 shepherd german toy dog great hit puppy really good lasted\n",
      "47 water tank fish gallon use clear filter change aquarium tanks\n",
      "48 cubes great chinchilla got ice love just bichon thing wheel\n",
      "49 cup suction thermometer tank cups duty accurate glass like work\n",
      "50 time got like used think new pup say ve recommend\n",
      "51 teeth dog dogs taste like flavor toothpaste brush loves breath\n",
      "52 time good just really day little like cat purchased long\n",
      "53 hair brush loose dog fur cat doesn use like does\n",
      "54 like dog thought expecting giant just really oh attractive does\n",
      "55 food cat using day use just eat meal lids used\n",
      "56 plants growing substrate sand just tank water good like aquarium\n",
      "57 treats dog dogs treat extreme pieces small just don kong\n",
      "58 did like water large just weren isn wasn looking drink\n",
      "59 vet cat food stomach cats day try days issues old\n",
      "60 air pump quiet hose flow tubing just filter great like\n",
      "61 usual used heard ve bought product strongly years day bad\n",
      "62 son traveling usa time cracked product like asked ingredients proven\n",
      "63 dog flea dogs product vet fleas frontline using works month\n",
      "64 little work way just ve sure bought getting got open\n",
      "65 chew dog toy chewer chewing toys dogs minutes chewed bone\n",
      "66 fight love round ve make time sure great periods residue\n",
      "67 prefer husky just smart dogs make dog siberian love ve\n",
      "68 turtles recommend great highly like works little just perfect got\n",
      "69 cats cat love like toy sturdy play pads grass string\n",
      "70 large don dog ends like size just small tend medium\n",
      "71 gsd picture bought male cat just open like years came\n",
      "72 old supplements just arthritis product supplement like pills hip help\n",
      "73 product like great did reviews use working didn got came\n",
      "74 warm cold night cat hours bed winter cover bought heat\n",
      "75 don like washed poo soap bought know really bit use\n",
      "76 smell smells product carpet puppy spray used miracle use stain\n",
      "77 like pocket training use clip belt just thing way crack\n",
      "78 idea cats mechanism self large breeze easily day does try\n",
      "79 use ve need going little dog allow bit time snow\n",
      "80 does thing don maybe think time use job great good\n",
      "81 litter box cat cats clean just use boxes like cleaning\n",
      "82 food fish eat feed love like feeding pellets healthy foods\n",
      "83 bird birds cage perch love seed perches parakeet seeds cages\n",
      "84 dog skin coat dogs oil soft food coats product dry\n",
      "85 ball balls throw tennis dog like chuckit yard throwing use\n",
      "86 toy favorite loves dog toys cat love play cats plays\n",
      "87 removes chlorine just does slime assume bottle chloramine long water\n",
      "88 clean great water couple day working cleaning problem use hard\n",
      "89 fish water product tank use days used dose sure adding\n",
      "90 just don did cheapest like master strips really lost think\n",
      "91 product joint use used bottle fault ve dog bought got\n",
      "92 great price cute good works dogs ferrets ferret small like\n",
      "93 got 20 just cheese long dog does lot work know\n",
      "94 just work sure really obviously certainly didn try good thing\n",
      "95 fleas plus frontline product flea ticks dogs used tick years\n",
      "96 dog yard walk took neighbors just don ve did didn\n",
      "97 food bag dog container bowl holds great size water easy\n",
      "98 product really works just water follow easy definately don pink\n",
      "99 cans dish food cat grain free canned cats feeding contain\n"
     ]
    }
   ],
   "source": [
    "def print_topic(topic_term, topic_id, vocabulary):\n",
    "    print(\"{:2d} \".format(topic_id) + \" \".join(vocabulary[i] for i in topic_term[i].argsort()[:-11:-1]))\n",
    "\n",
    "for i, _  in enumerate(topic_term):\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the topics are reasonably coherent, so I'm going to move on.\n",
    "\n",
    "If things look messy:\n",
    " - n_topics might be too large, either for the diversity in the corpus (maybe there really aren't 1000 topics), or for the number of documents you have (you just don't have enough data)\n",
    " - n_topics might be too low (real topics have to be merged together by the algorithm, which doesn't work well)\n",
    " - you've got a bug!\n",
    " \n",
    "Setting n_topics very small (say 5) or very high (say 1000) is a good way of building up some intuition for what works (although beware `n_topics=1000` will take a long time to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect a document in the corpus\n",
    "\n",
    "The topic model is the lens through which we're going to view future documents.\n",
    "\n",
    "But let's first look at our existing documents through this lens.\n",
    "\n",
    "To do that we have to transform the documents we trained on to be distributions of topics (e.g. document 1 is 20% topic A, 30% topic B, etc.)\n",
    "\n",
    "We do that by running the `lda.transform` method on the vectorized documents `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_topic` has a row for each document, and a column for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, each row should add up to 1, and again they don't because of a bug in scikit-learn, so let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic /= doc_topic.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's look at the topic distribution of a random document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My toy poodle loves this stuff and will let me &#34;sort&#34; of brush her teeth because of it.  I was hoping it would help with her doggy breath and it does some.  Interestingly... it says &#34;peanutbutter&#34; but it doesn't smell like peanutbutter.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.loc[7, 'reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51 84 57 59 44]\n"
     ]
    }
   ],
   "source": [
    "top_topics = (doc_topic[7]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 teeth dog dogs taste like flavor toothpaste brush loves breath\n",
      "84 dog skin coat dogs oil soft food coats product dry\n",
      "57 treats dog dogs treat extreme pieces small just don kong\n",
      "59 vet cat food stomach cats day try days issues old\n",
      "44 says product use does wrong killed good tearing alive time\n"
     ]
    }
   ],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(topic_term, i, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis is a comprehensive package for visualizing the results of a topic model. It's useful for understanding the structure of the model you've just discovered. The topics exist in a huge space. This package squeezes things down to 2D so we can look at it on the screen.\n",
    "\n",
    "In my experience, it generates a ton of spurious warnings, so let's disable warnings for this package when we import it (a useful trick!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    try:\n",
    "        import pyLDAvis\n",
    "    except ImportError:\n",
    "        print('ERROR: pyLDAvis not installed! Skip to next section!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `topic_term` and `doc_topic` matrices, pyLDAvis needs to know how often each word occurs in the entire corpus, and how long each document is. Here are calculations that give those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_frequency = np.asarray(X.sum(axis=0)).squeeze()\n",
    "doc_lengths = [len(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.prepare(topic_term_dists=topic_term,\n",
    "                           doc_topic_dists=doc_topic,\n",
    "                           doc_lengths=doc_lengths,\n",
    "                           vocab=vocabulary,\n",
    "                           term_frequency=term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all this together in a Pipeline and persist the model\n",
    "\n",
    "The process of getting from document to topic distribution is a little fiddly. We need to:\n",
    " - Vectorize the document (using the same vocabulary we used when training above)\n",
    " - Transform the document using the LDA object\n",
    " \n",
    "scikit-learn allows us to bundle these steps (and more!) together in an object called a `Pipeline`, which we can save to disk, reload, and work with again. Let's build one, train it, and save it.\n",
    "\n",
    "**WARNING**: this next cell will take a while to execute the first time you run it. After that though, the model will be loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "try:\n",
    "    with open('topic_model.pkl', 'rb') as f:\n",
    "        topic_pipeline = pickle.load(f)\n",
    "    pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()\n",
    "except IOError:\n",
    "    topic_pipeline = make_pipeline(\n",
    "        CountVectorizer(stop_words='english', binary=True, max_features=10000),\n",
    "        LatentDirichletAllocation(n_topics=100, learning_method='batch', n_jobs=-2, random_state=0)\n",
    "    )\n",
    "    topic_pipeline.fit(texts)\n",
    "    with open('topic_model.pkl', 'wb') as f:\n",
    "        pickle.dump(topic_pipeline, f)\n",
    "        \n",
    "pipeline_vocabulary = topic_pipeline.steps[0][1].get_feature_names()\n",
    "pipeline_topic_term = topic_pipeline.steps[1][1].components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine topics of a new document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single document we looks at above was a pretty short document. Let's make a more interesting, longer document out of all the reviews of that product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomreviews = \" \".join(texts[1100:1105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Careful people! Even though this chew toy says for powerful chewers, it also says for dogs up to 50 lbs in VERY tiny print on the bottom of the package. I gave it to my 79 lb Lab Mix puppy anyway, since I had already bought it... and lo and behold, it began falling apart in only a few moments.After removing the toy and picking up the white bits all over... I thought to come here and warn people. I find it odd how it's for powerful chewers yet only includes medium sized dogs in the toy description! With pit bull terriers and a few other smaller breeds as an exception... usually smaller dog means less destructive chewing ability. Senseless, really.If you have a dog that falls under this weight category... except for a breed with very powerful jaws, this toy probably suits just fine, but if you have a large dog, don't even think about it. I paid almost $9 for a toy that begun to disintegrate within a few seconds. I bought the Nylabone dinosaur chew toy for my 80lb. Greyhound and instead my 20lb Bichon has taken charge of it.  He chews on it several times a day.  The Greyhound doesn't stand a chance getting to it! I was disappointed in this one - Nylabone Durable toys are the only toys I buy for my powerful chewers because they are the most durable I've found to withstand their chewing. This one did not live up to the Nylabone Durable name. I gave it to one of my dogs, who is not the most aggressive chewer in the pack and I had to take it away from him the first day because he was able to destroy part of it and chew off pieces.  I will go back to the Nylabone Durable Souper bones from now on! It all depends on whether your dog likes it or not. My friend's dog chews up toys within a day or two of getting them. He has this one still after several months. The head is gone, but the rest is intact. My boyfriend's dog, however, only picks it up when she knows I'm waiting for her to play with it. My only complaint is that when your dog is carrying it around, if he scrapes up against you with the little nubs can be pretty sharp. We have two lab mixes. The youngest is still very much in puppy stage and boy does he love to chew! We had given him rawhides to keep his focus on something appropriate to chew, but he was going through too many rawhides. I saw nylabones in petsmart in the heavy chewers section. The first plain bone went over well. So I ordered more on Amazon. I'm so glad I did! These were the best solution to keep our chewer occupied! He loves all the different shapes and flavor toys that they have! Thank goodness for nylabones!\n"
     ]
    }
   ],
   "source": [
    "print(randomreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = topic_pipeline.transform([randomreviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "print(doc_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65 31 70 53 86]\n"
     ]
    }
   ],
   "source": [
    "top_topics = (doc_topic[0]).argsort()[:-6:-1]\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 chew dog toy chewer chewing toys dogs minutes chewed bone\n",
      "31 chew nylabone puppy dog dogs loves chews chewing like bones\n",
      "70 large don dog ends like size just small tend medium\n",
      "53 hair brush loose dog fur cat doesn use like does\n",
      "86 toy favorite loves dog toys cat love play cats plays\n"
     ]
    }
   ],
   "source": [
    "for i in top_topics:\n",
    "    print_topic(pipeline_topic_term, i, pipeline_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What next?\n",
    "\n",
    "Use the `doc_topic` array for a downstream task, e.g.\n",
    " - corpus exploration (remember the [NYT visualization](http://christo.cs.umass.edu/NYT/))\n",
    " - document clustering, e.g. use something like `KMeans` (in scikit-learn) to visualize which documents are most similar in terms of their topics, which may surface groups of topics or groups of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarization\n",
    "\n",
    "Here's a short algorithm, but see [Fast Forward Labs Report 04](http://ff04.fastforwardlabs.com) for details:\n",
    "  - Train LDA on all products of a certain type (e.g. all the books)\n",
    "  - Treat all the reviews of a particular product as one document, and infer their topic distribution\n",
    "  - Infer the topic distribution for each sentence\n",
    "  - For each topic that dominates the reviews of a product, pick some sentences that are themselves dominated by that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Be aware of limitations:\n",
    " - Choosing `n_topics` is an art rather than a science!\n",
    " - The topics don't come with names. Sometimes they overlap. Sometimes they're not what you want them to be. For example, if you run a topic model on the NYT corpus, there's no guarantee you'll get topics that correspond to the sections of the newspaper (business, metro, world, sport, etc.!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The way you used `fit` and `transform` for both the `vectorizer`, `lda`, and `topic_pipeline` objects is generic across scikit-learn, so play with scikit-learn, e.g. [Andreas Mueller's presentation](https://www.youtube.com/watch?v=8CzwlZbwDkI) is a good place to start.\n",
    " \n",
    "Remember if you're interested in the LDA algorithm itself, take a look at\n",
    "\n",
    " - [Tim Hopper's PyData NYC 2015 talk](https://www.youtube.com/watch?v=_R66X_udxZQ)\n",
    " - [David Blei's ACM article](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
